1.sklearn的knn函数参数说明
    !!!官网地址：https://scikit-learn.org/dev/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier

    在我看来：目前我理解的，
        一个是n_neighbors参数，其含义为k值的大小，
        另一个为weights，指定一个weight可以让模型分类有一个依据，其中distance是距离的反比；


    KNeighborsClassifier( n_neighbors=5, weights=’uniform’, algorithm=’auto’, leaf_size=30, p=2, metric=’minkowski’, metric_params=None, n_jobs=1, **kwargs )

    1)n_neighbors，即选择k值，较大的k值可以容忍较多的噪声，但是会使得分类的结果变的不那么精确；但样本不平衡时，一个类样本容量很大，其他样本容量很小，导致输入新样本时，该样本邻近中大样本占多数，因此可以采取权值的方法（和样本距离小的邻近权值大）来改进，减少k值选择对结果的影响；可以通过循环来尝试不同的k值的效果。
    2)weights，即权重设置，‘uniform’和‘distance’，对每个点赋予同样的权重，或者根据距离远近，赋予不同的权重（距离越远权重越小）。
    3)algorithm，即搜索最近邻算法的选择，包括ball_tree、kd_tree、brute和auto。
    4)brute代表Brute Force算法，就是一个最原始（蛮力）邻居搜索做法，Brute Force在寻找邻居时，会把输入的点同所有样本中的点做一个距离计算，然后再排序选择最近的点。
    5)kd_tree代表KD树算法，为了解决BF算法效率低下的问题，人们提出了很多树形结构的算法，树形结构的主要贡献在于，它们一般通过事先的合理组织，降低了我们搜索中的开销，也就是使用树形结构的算法，我们在搜索时可以不用每个样本都去计算距离了，这无疑降低了很多的开销。在不太高的维度（小于20）下，KD树的效果是很不错的，不过当维度走向更高时，KD树的效率也会随之下降，这也呼应了我们之前提到的“维度之殇”的问题。
    6)ball_tree代表Ball Tree算法，KD-Tree是为了解决Brute Force的效率问题，那么Ball-Tree也就是为了解决KD-Tree在高维情况下的效率不佳的另一个做法了，Ball树在构建的时候，比起KD树要复杂许多，不过在最后的搜索过程中，其表现就会非常好。
    三者的选取：样本≤30，用BF；样本较大，但维度≤20，用KD_tree；维度更多的，用ball tree。
    7)leaf_size，设置一个数值，什么时候切换到BF暴力求解，默认是30。
    8)metric ，用于树的距离度量。默认'minkowski与P = 2（即欧氏度量）。

